import numpy as np
import matplotlib.pyplot as plt
import random

x = np.array([ x+random.uniform(0,1) for x in range(0,10)])
y = np.array([ y+random.uniform(0,1) for y in range(0,10)])

#Compute prediction
def prediction(w,b,x):
  return w*x+b

n = len(y) 

#Compute loss function and gradient's
def loss_function(y,y_p):
  return (1/n)* sum([val**2for val in (y-y_p)])

def w_slope(y,y_p,x):
  return -(2/n)*sum(x*(y-y_p))

def b_slope(y,y_p,x):
  return -(2/n)*sum(y-y_p)

#Define new Steps to change the values of w & b
def w_new(y,y_p,x,w,lr):
  return w - lr* w_slope(y,y_p,x)
def b_new(y,y_p,x, b,lr):
  return b - lr* b_slope(y,y_p,x)

#Starting values for w & b
def gradient_descent(y,x,w,b, lr):

  y_prediction = []
  w_values=[]
  b_values=[]
  loss_values = []

  iterations = 1000
  for i in range(iterations):
    y_pred = prediction(w,b,x)
    w = w_new(y,y_pred,x,w,lr)
    b = b_new(y,y_pred,x,b,lr)
    Loss = loss_function(y,y_pred) 

    y_prediction.append(y_pred)
    w_values.append(w)
    b_values.append(b)
    loss_values.append(Loss)

    print(f'w={w}, b={b}, loss = {loss_function(y,y_pred)}, i = {i}')

if __name__ == '__main__':
  w=b=0
  lr=0.0259
  gradient_descent(y,x,w,b,lr)
